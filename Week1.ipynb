{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Week 1: AI-Based Garbage Classification - Updated Code\n",
    "\n",
    "# Step 1: Install required libraries\n",
    "!pip install -q kaggle\n",
    "!pip install -q tensorflow\n",
    "!pip install -q matplotlib\n",
    "\n",
    "# Step 2: Upload kaggle.json for API access\n",
    "from google.colab import files\n",
    "files.upload()  # Upload kaggle.json manually\n",
    "\n",
    "# Step 3: Set up Kaggle API and download dataset\n",
    "import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/content\"\n",
    "!chmod 600 /content/kaggle.json\n",
    "!kaggle datasets download -d farzadnekouei/trash-type-image-dataset\n",
    "!unzip -oq trash-type-image-dataset.zip -d data\n",
    "\n",
    "# Step 4: Import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 5: Prepare dataset paths (corrected path)\n",
    "train_path = \"data/TrashType_Image_Dataset\"\n",
    "val_path = \"data/TrashType_Image_Dataset\"  # Using same path as there's no split\n",
    "\n",
    "# Step 6: Create ImageDataGenerator instances\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_path,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Step 7: Build CNN Model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(6, activation='softmax')  # 6 classes\n",
    "])\n",
    "\n",
    "# Step 8: Compile Model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 9: Train Model\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=10)\n",
    "\n",
    "# Step 10: Plot accuracy and loss\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
